## Step 1 - Chop time

### 1. timechop.Timechop

Input:
* Temporal config (update window, label windows, modeling start, etc)

Output:
* List of train/test splits, in dict form

## Steps 2-4 - Create pre-matrix tables

Labels, features, states. What they all have in common is that they take in some staging table, the experiment's as of dates, some config, and create a table scoped to the experiment's as of dates containing data that is ready to put into a matrix

### 2. architect.StateTableGenerator

Input:
* as of dates (from the experiment)
* table containing either events or 'dense states' (entity_id/state/start/end)

Output:
* table containing 'sparse states', (a row for each entity id and as_of_date, plus a boolean for each state) for all as_of_dates in the experiment


### 3. architect.LabelGenerator

Input:
* events/outcomes table (entity id, outcome date, outcome) representing the outcomes you want to generate experiment labels from
* as of dates (from the experiment)
* label windows

Output:
* table containing labels for all as of dates in the experiment


### 4. architect.FeatureGenerator

Input:
* tables containing data that is suitable for features
* collate aggregation definitions
* as of dates (from the experiment)

Output:
* tables containing features for all as of dates in the experiment

## Steps 5-8 - Matrix Preparation

These are all computationally-light tasks that involve taking our experiment configuration and the tables that have been created already, and figuring out what matrices need to be made. These steps are ordered.

### 5. architect.FeatureDictionaryCreator

Input:
* names of feature tables

Output:
* dictionary: keys are all the table names of the features that the FeatureGenerator spit out, values are the names of the feature columns within that table. Not all features in here necessarily end up in any matrices, but it is meant to be a superset of all possible features we might create

### 6. architect.FeatureGroupCreator

Input:
* master feature dictionary (from FeatureDictionaryCreator)
* rules for generating feature groups (currently just a list of tables and/or collate prefixes)

Output:
* list of smaller feature dictionaries. Same structure as the one that comes out of FeatureDictionaryCreator, but each one representing the features in a group

### 7. architect.FeatureGroupMixer

Input:
* list of feature dictionaries (from FeatureGroupCreator)
* rules for mixing feature groups (currently leave-one-out, leave-one-in, all)

Output:
* list of yet smaller feature dictionaries, still same structure


### 8. architect.Planner
Input:
* directory where matrices are stored
* list of states to iterate through (from experiment config)
* list of label types to iterate through (from experiment config)
* list of label names to iterate through (from experiment config)
* list of feature dictionaries to iterate through (from FeatureGroupMixer)
* list of matrix split definitions (from Timechop)

Output:
* list of 'build tasks', one for each unique matrix we need to build. A dictionary representing a small payload-serializable definition of what is needed to build that matrix. These are:
	* as of dates
	* label name
	* label type
	* feature dictionary
	* matrix directory
	* matrix uuid (generated by metta)
	* matrix metadata (in format specified by metta. ends up in the matrix's .yaml file)
	* matrix type (train or test)

## Step 9. Building the matrices

Although there's only one component here, it needs quite a bit of description. This component may need some breaking up. Generally, it creates train or test matrices via a number of join queries. It creates an entity-date table, either using all valid and labeled entity dates (for train) or all valid entity dates (for test). It extracts labels and features table-by-table by joining into individual files, and then merges them into one big dataframe. This is also where the feature zero-imputation happens (interpolated into the extraction query).

### 9. architect.Builder

Input:
* a build task from Planner (or list of them)
* the completed states, labels, and feature tables from steps 2-4
* (soon): format/medium details for writing the finished matrix, like CSV/HDF5 or disk/S3?

Output:
* matrix and metadata file

## Steps 10-13: Run models (catwalk)

### 10. catwalk.ModelTrainer

Input:
* a place to store a trained model
* a train matrix
* a classpath to something that implements the sklearn 'fit' interface
* a hyperparameters

Output:
* trained model object
* metadata in the results.model_groups, results.models, results.feature_importances tables


### 11. catwalk.Predictor

Input:
* trained model
* prediction matrix

Output:
* predictions_proba
* metadata in the results.predictions table

### 12. catwalk.ModelEvaluator

Input:
* predictions_proba
* labels
* model id
* evaluation start
* evaluation end
* example frequency
* metric grid (dictionary with metrics and thresholds to compute)

Output:
* metadata in the results.evaluations table

### 13. (soon) catwalk.IndividualImportance

Input:
* model id
* prediction matrix
* as of dates (from the experiment)
* individual importance method

Output:
* metadata in the results.individual_importances table
